---
title: "Lab #9: Dictionary-Based Text Analysis"
author: "Data Science and Society (Sociology 367)"
date: ""
output: html_document
---

**[Lab #9 Markdown File](/assignments/Lab_9.Rmd)**

**[Lab Instructions](https://dssoc.github.io/assignments/lab_instructions.html)**

In this lab, we will practice working with dictionary-based methods for text analysis.

See the "Instructions" section of the [Introduction to Lab Assignments](https://dssoc.github.io/assignments/lab_instructions.html) page for more information about the labs. That page also gives descriptions for the datasets we will be using.

**Required reading:** 

+ Text Mining with R: [The tidy text format](https://www.tidytextmining.com/tidytext.html)
+ Text Mining with R: [Analyzing word and document frequency: tf-idf](https://www.tidytextmining.com/tfidf.html)
* R for Data Science: [Working with strings (Chapter 14)](https://r4ds.had.co.nz/strings.html)


**Optional reading:** 

* [stringr package docs](https://www.rdocumentation.org/packages/stringr/versions/1.4.0)
* [tidytext package docs](https://www.rdocumentation.org/packages/tidytext/versions/0.2.6)
* Test and develop regex expressions on [regexr.com](https://regexr.com/)



```{r message=FALSE, echo=FALSE}
library(tidyverse)
library(stringr)
library(tidytext)
library(tm)
library(lubridate)

load(url('https://dssoc.github.io/datasets/senator_tweets.RData'))
load(url('https://dssoc.github.io/datasets/congress.RData'))

```
<br/>



## Questions
<br>


**1. In which scenarios would it be best to consider dictionary-based approaches to text analysis? How does the decision to use dictionary-based approaches shape the research questions you can ask?**
```
# your answer here
```
<br/>

**2. Create a bar graph showing the frequencies of the twenty most-used tokens in our `senator_tweet_sample` corpus after removing URLs and stopwords, but preserving hashtags as tokens (e.g. "#19thamendment" should be a single token). Now create a similar plot that ONLY includes the hashtags.**

Hint: you can do hashtag preservation in many ways, but you might find an easy solution by browsing the documentation for [`unnest_tokens`](https://www.rdocumentation.org/packages/tidytext/versions/0.2.6/topics/unnest_tokens
) - see the `token` parameter. Searching on the internet may also be a good strategy.
```{r}
# your answer here
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

frequent_tokens <- senator_tweet_sample %>%
  mutate(text = str_remove(text, url_pattern)) %>%
  unnest_tokens("word", text, token = "tweets") %>% 
  anti_join(stop_words) %>%
  count(word) %>%
  arrange(desc(n))

frequent_tokens %>% 
  head(20) %>% 
  ggplot(aes(y = reorder(word, n), x=n)) +
  geom_bar(stat = "identity") +
    xlab("") +
    ylab("Frequency")

frequent_tokens %>% 
  filter(startsWith(word, "#")) %>% 
  head(20) %>% 
  ggplot(aes(y = reorder(word, n), x=n)) +
  geom_bar(stat = "identity") +
    xlab("") +
    ylab("Frequency")
```
<br/>

**3. For each of the top three most frequent non-stopword tokens, extract up to three tweets with the highest number of retweets that include the token. Based on the context provided in these Tweets, give a quick sentence about how they seem to be used in this context.**

HINT: it might be useful to use `str_count` here.
```{r}
# your answer here

url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

tf_idfs <- senator_tweet_sample %>%
  mutate(text = str_remove(text, url_pattern)) %>%
  unnest_tokens("word", text, token = "tweets") %>% 
  anti_join(stop_words) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n) %>% 
  arrange(desc(tf_idf))

tf_idfs %>% 
  head(10) %>% 
  ggplot(aes(y=reorder(word, tf_idf), x=tf_idf)) +
    geom_bar(stat='identity')
```
```
your written explanation here
```
<br/>

**4. Create a bar graph showing the tf-idf scores of the ten tokens with the highest values in our corpus, again preserving hashtags as tokens and removing urls/stopwords. What do these scores mean? Give a hypothesis for why the top three have the highest values.**
```{r}
# your answer here
toppers <- tf_idfs %>% 
  head(3)

senator_tweet_sample %>% 
  arrange(retweet_count) %>% 
  mutate(ct1=str_count(tolower(text), fixed(toppers$word[[1]]))) %>% 
  mutate(ct2=str_count(tolower(text), fixed(toppers$word[[2]]))) %>% 
  mutate(ct3=str_count(tolower(text), fixed(toppers$word[[3]]))) %>% 
  filter((ct1+ct2+ct3) > 0)
```
```
your written explanation here
```
<br/>


**5. Create a new column in senator_tweet_sample that corresponds to the time of day that a given tweet was posted, and make a bar graph comparing the number of tweets published in day (5am-5pm) vs night.**

Hint: see the `hour` function of lubridate.
```{r}
# your answer here
senator_tweet_sample %>% 
  mutate(hr=hour(created_at), is_day=(hr >= 5) & (hr <= 17)) %>% 
  mutate(tod=ifelse(is_day, 'Daytime', 'Nighttime')) %>% 
  count(tod) %>% 
  ggplot(aes(x=tod, y=n)) +
    geom_bar(stat='identity')
```
<br/>


**6. Use the "bing" sentiment dictionary to compare the average sentiment for Tweets published in daytime vs nighttime using a bar plot. You get to choose how you will create these sentiment scores for comparison - explain and justify your decision. Also explain your interpretation of the results.**

HINT: use `get_sentiments("bing")` to get the Bing dictionary.
```{r}
# your answer here

daynight_sentiment_summarize <- function(tweets) {
  
  # filter and apply dictionary
  word_sents <- tweets %>%
    mutate(hr=hour(created_at), is_day=(hr >= 5) & (hr <= 17)) %>% 
    mutate(tod=ifelse(is_day, 'Daytime', 'Nighttime')) %>% 
    mutate(text = str_remove(text, url_pattern)) %>%
    unnest_tokens("word", text, token = "tweets") %>% 
    anti_join(stop_words, by='word') %>%
    filter(!startsWith(word, '@')) %>% 
    left_join(get_sentiments("bing")) %>% 
    select(status_id, tod, word, sentiment)
  
  # average within status and then within time-of-day
  sent_summary <- word_sents %>% 
    group_by(status_id) %>% 
    summarize(
      tod=first(tod), 
      percent_pos=sum(sentiment=='positive', na.rm=T)/n(), 
      percent_neg=sum(sentiment=='negative', na.rm=T)/n()
    ) %>% 
    group_by(tod) %>% 
    summarize(Positive=mean(percent_pos), Negative=mean(percent_neg)) %>% 
    pivot_longer(Positive:Negative, names_to='Sentiment', values_to='Average')

  return(sent_summary)
}

daynight_sentiment_summarize(senator_tweet_sample) %>% 
  ggplot(aes(x=Average, y=Sentiment, fill=tod)) +
    geom_bar(stat='identity', position='dodge')
  
```
```
Explain why you chose to compute sentiment in this way.
I just used raw counts instead of averages. Feel free to use whatever measure you find to be helpful.
```
<br/>


**7. Create a custom dictionary with at least two categories (e.g. positive/negative, happy/sad, solution/problem-oriented) and compare daytime-nightime scores for each of the two categories. What does this result tell you about your data? What is your dictionary capturing here?**

Hint: you may want to look at the bing dictionary (`get_sentiments("bing")`) to see how you should format your custom dictionary.
```{r}
# your answer here
custom_sentiment <- data_frame(happy=c('joy', 'exicted', 'happy'), sad=c('sad', 'dissappointed', 'angry')) %>% 
  pivot_longer(happy:sad, values_to='word', names_to='sentiment')


daynight_custom_summarize <- function(tweets) {
  
  # filter and apply dictionary
  word_sents <- tweets %>%
    mutate(hr=hour(created_at), is_day=(hr >= 5) & (hr <= 17)) %>% 
    mutate(tod=ifelse(is_day, 'Daytime', 'Nighttime')) %>% 
    mutate(text = str_remove(text, url_pattern)) %>%
    unnest_tokens("word", text, token = "tweets") %>% 
    anti_join(stop_words, by='word') %>%
    filter(!startsWith(word, '@')) %>% 
    left_join(custom_sentiment, by='word') %>% 
    select(status_id, tod, word, sentiment)
  
  
  
  # average within status and then within time-of-day
  sent_summary <- word_sents %>% 
    group_by(status_id) %>% 
    summarize(
      tod=first(tod), 
      percent_pos=sum(sentiment=='happy', na.rm=T)/n(), 
      percent_neg=sum(sentiment=='sad', na.rm=T)/n()
    ) %>% 
    group_by(tod) %>% 
    summarize(Happy=mean(percent_pos), Sad=mean(percent_neg)) %>% 
    pivot_longer(Happy:Sad, names_to='Sentiment', values_to='Average')

  return(sent_summary)
}

daynight_custom_summarize(senator_tweet_sample) %>% 
  ggplot(aes(x=Average, y=Sentiment, fill=tod)) +
    geom_bar(stat='identity', position='dodge')

```

```
Explain what your dictionary is intended to capture and interpret the results.
```
<br/>

**8. Using the data you have collected for your final project, show one preliminary result or statistic from an analysis you ran. If you haven't collected your dataset computationally, try to look anecdotally at the original source (e.g. if Twitter is your dataset, then just look on the Twitter website) and give one observation about the data. Try to make an observation or result based on one of the variables you will use for your final analysis. What do you see? Send your figures and statistics directly to your TA in Slack - don't add them to your script.**
```
written description here
```
<br/>

