---
title: "Lab #10: Your First Topic Model"
author: "Data Science and Society (Sociology 367)"
date: ""
output: html_document
---

**[Lab #10 Markdown File](/assignments/Lab_10.Rmd)**

**[Lab Instructions](https://dssoc.github.io/assignments/lab_instructions.html)**

In this lab, we will practice working with topic modeling algorithms. We will use a dataset of wikipedia pages for each senator, and explore the ways topic modeling can help us learn about the corpus. First we will use the LDA algorithm to practice the basics of topic modeling, then we will use the structural topic modeling algorithm (see the `stm` package) to show how we can use information about each senator (age, gender, political party) in conjunction with our model. **NOTE: if you run into problems where your code takes too long to run or your computer freezes up, use the `substr` function to truncate the wikipedia page texts right after loading.**

See the "Instructions" section of the [Introduction to Lab Assignments](https://dssoc.github.io/assignments/lab_instructions.html) page for more information about the labs. That page also gives descriptions for the datasets we will be using.


**Required reading:** 

* Text Mining with R: A Tidy Approach, [chapter 6: Topic Modeling by Julia Silge and David Robinson](https://www.tidytextmining.com/topicmodeling.html)
* [IMPORTANT stm Package Vignette](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf)



**Optional reading:** 

* [An intro to topic models by Patrick van Kessel](https://medium.com/pew-research-center-decoded/an-intro-to-topic-models-for-text-analysis-de5aa3e72bdb)

* [topicmodels package docs](https://cran.r-project.org/web/packages/topicmodels/topicmodels.pdf)
* [tm package docs](https://cran.r-project.org/web/packages/tm/tm.pdf)
* [stringr package docs](https://www.rdocumentation.org/packages/stringr/versions/1.4.0)
* [tidytext package docs](https://www.rdocumentation.org/packages/tidytext/versions/0.2.6)
* [stm website](https://www.structuraltopicmodel.com/)



**Load the datasets and libraries. You shouldn't need to change the URL in the `load` function**
```{r message=FALSE}
library(tidytext)
library(tidyverse)
library(tidyr)
library(dplyr)
library(tm)
library(stringr)
library(topicmodels)
library(ggplot2)
library(stm)


load(url('https://dssoc.github.io/datasets/congress.RData'))
load(url('https://dssoc.github.io/datasets/senator_wiki.RData'))

```
<br/>



## Questions
<br>


**1. Describe a document-term matrix (DTM) in your own words. Why is this data structure useful for text analysis?**
```
your answer here
```
<br>

**2. Describe a topic modeling algorithm in your own words. What is the input to a topic modeling algorithm (after parsing the raw text)? What does the actual topic model look like? How do you choose the number of topics? What are the beta parameter estimates?**
```
your answer here
```
<br>

**3. Join the columns of `congress` with `senator_wiki` (use `inner_join`) so that you should have Senator member information (gender, age, etc) associated with every wikipedia page in our dataset, then create a document-term matrix from `text` or `subtext` (whichever is easier for you to use) after removing stopwords.**

HINT: your merged dataframe should have as many rows as the original `senator_wiki` dataframe.

HINT: check out the `cast_dtm` function if you're using `unnest_tokens`. Recall that we created a DTM as part of Lab 8 as well.
```{r}
# your answer here
wiki_info <- congress %>% inner_join(senator_wiki, by='bioguide_id')
dtm <- wiki_info %>% 
  select(bioguide_id, subtext) %>% 
  unnest_tokens('word', 'subtext') %>% 
  anti_join(stop_words) %>% 
  count(bioguide_id, word) %>% 
  cast_dtm(bioguide_id, word, n)
```
<br>

**4. Construct a topic model with LDA using a specified random seed (see the `control` parameter). This might take a little while to run. You can choose the number of topics however you see fit - it might be useful to try multiple values.**
```{r}
# your answer here
tm <- LDA(dtm, k=10, control=list(seed=0))
```
<br>

**5. Make a function that accepts (takes as an argument) a LDA topic model and returns a plot showing the word distributions for the top ten words in each topic, then call the function with your topic model. Choose two topics which appear to be easiest to understand, and explain what you think they represent based on the word distributions.**
```{r}
# your answer here
plot_topwords <- function(tm) {
  topwords <- tm %>% 
    tidy() %>% 
    group_by(topic) %>% 
    top_n(10, beta) %>% 
    ungroup() %>% 
    arrange(topic, -beta)
  plot <- topwords %>% ggplot(aes(x=reorder(term, beta), y=beta, fill=(topic))) + 
    geom_col(show.legend=FALSE) +
    facet_wrap(~topic, scales='free') +
    theme(plot.title=element_text(hjust=0.5, size=18)) +
    xlab("") + ylab("") +
    theme_minimal() +
    coord_flip()
  return(plot)
}
plot_topwords(tm)
```
```
your answer here
```
<br>

**6. Create a structural topic model with the `stm` package using politician gender, political party affiliation, and (approximate) age as covariates in the model. Then use `plot` with no parameters to show prevalence and top words associated with each topic.**

HINT: to create the STM, start with Section 3.1-3.3 of the stm Package Vignette listed in the required readings. You'll use `textProcessor`, `prepDocuments`, and then `stm` to create the STM topic model.

```{r}
# your answer here
covariates <- wiki_info %>% 
  mutate(age=2022-birthyear) %>% 
  select(age, gender, party)
  
processed <- textProcessor(wiki_info$subtext, metadata=covariates)
prep <- prepDocuments(processed$documents, processed$vocab, processed$meta)
formula <- ~ age + gender + party
wiki_stm <- stm(documents=prep$documents, vocab=prep$vocab, data=prep$meta, 
                prevalence = formula, 
                K=10, verbose=FALSE)
plot(wiki_stm)
```
```
your answer here
```
<br>



**7. Use `labelTopics` to view the words associated with two topics you find most interesting. Can you easily describe what these topics are capturing?**

HINT: to use `labelTopics`, read Section 3.5 of the stm Package Vignette.
```{r}
# your answer here
labelTopics(wiki_stm, c(1, 8, 7))
```
```
your answer here
```
<br>


**8. Now we will try to understand how each of our covariates (politician age, gender, and political party) corresponds to each topic. This is done primarily through use of the `estimateEffect` function. Use `estimateEffect` and `summary` to print out models corresponding to each of our topics. Identify several situations where a covariate is predictive of a topic. Then, create a plot showing those effect sizes with confidence intervals using the `plot` function. Make sure the figure is readable. Which topics are the most interesting based on the covariate significance? What do these results tell you?**

HINT: See the plot in section 3.6 of the stm Package Vignette under the heading "Topical content" on pages 18-19.
```{r}
# your answer here
pred <- estimateEffect(formula = 1:10 ~ gender + party + age,
                       stmobj = wiki_stm, metadata = prep$meta, uncertainty="Global")
pred %>% summary()

# just example - none of these are statistically significant
pred %>% plot(covariate='gender', topics=c(1, 3, 4), model=wiki_stm, method='difference',
     cov.value1 = 'F', cov.value2 = 'M')

pred %>% plot(covariate='party', topics=c(7, 10), model=wiki_stm, method='difference',
     cov.value1 = 'Democrat', cov.value2 = 'Republican')

pred %>% plot(covariate='age', topics=c(1, 7, 8), model=wiki_stm, method='continuous')

# if we have specific ages to compare, we can use difference method instead of continuous
pred %>% plot(covariate='age', topics=c(1, 7, 8), model=wiki_stm, method='difference',
     cov.value1 = 50, cov.value2 = 90)
```
```
your answer here
```
<br>


**9. Before this assignment is due, make a short post about your final project in the `#final-project-workshop` channel in Slack, and give feedback or helpful suggestions to at least one other project posted there. This will be a good way to receive and offer help to your peers!**
<br>


**Congratulations! This is the last lab for the course. These labs were not easy, but you persisted and I hope you learned a lot in the process. As you probably noticed by now, learning data science is often about trying a bunch of things and doing research on the web to see what others have done. Of course, it also requires a bit of creativity that comes from experience and intuition about your dataset. Be sure to talk to Professor Bail and the TA to make sure you're on the right track for the final project. Good luck!**

